---
apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  annotations:
    opendatahub.io/accelerator-name: migrated-gpu
    opendatahub.io/apiProtocol: REST
    opendatahub.io/hardware-profile-name: ""
    opendatahub.io/recommended-accelerators: '["nvidia.com/gpu"]'
    opendatahub.io/template-display-name: vLLM Multi-Node ServingRuntime for KServe
    openshift.io/display-name: vllm-multinode-runtime
  labels:
    opendatahub.io/dashboard: "true"
  name: vllm-multinode-runtime
  namespace: vllm
spec:
  annotations:
    prometheus.io/path: /metrics
    prometheus.io/port: "8080"
  containers:
  - args:
    - "ray start --head --disable-usage-stats --include-dashboard false \n# wait for
      other node to join\nuntil [[ $(ray status --address ${RAY_ADDRESS} | grep -c
      node_) -eq ${PIPELINE_PARALLEL_SIZE} ]]; do\n  echo \"Waiting...\"\n  sleep
      1\ndone\nray status --address ${RAY_ADDRESS}\n\nexport SERVED_MODEL_NAME=${MODEL_NAME}\nexport
      MODEL_NAME=${MODEL_DIR} \n\nexec python3 -m vllm.entrypoints.openai.api_server
      --port=8080 --max-model-len=100000 --distributed-executor-backend ray --model=${MODEL_NAME}
      --served-model-name=${SERVED_MODEL_NAME} --tensor-parallel-size=${TENSOR_PARALLEL_SIZE}
      --pipeline-parallel-size=${PIPELINE_PARALLEL_SIZE} --disable_custom_all_reduce\n"
    command:
    - bash
    - -c
    env:
    - name: RAY_USE_TLS
      value: "1"
    - name: RAY_TLS_SERVER_CERT
      value: /etc/ray/tls/tls.pem
    - name: RAY_TLS_SERVER_KEY
      value: /etc/ray/tls/tls.pem
    - name: RAY_TLS_CA_CERT
      value: /etc/ray/tls/ca.crt
    - name: RAY_PORT
      value: "6379"
    - name: RAY_ADDRESS
      value: 127.0.0.1:6379
    - name: POD_IP
      valueFrom:
        fieldRef:
          fieldPath: status.podIP
    - name: POD_NAMESPACE
      valueFrom:
        fieldRef:
          fieldPath: metadata.namespace
    - name: VLLM_NO_USAGE_STATS
      value: "1"
    - name: HOME
      value: /tmp
    - name: HF_HOME
      value: /tmp/hf_home
    image: quay.io/modh/vllm@sha256:79e1f24bba1d3e694f47f66ba9f8184e70310a10b77bf11c0febd0c926234950
    livenessProbe:
      exec:
        command:
        - bash
        - -c
        - "# Check if the registered ray nodes count is greater or the same than PIPELINE_PARALLEL_SIZE\nregistered_node_count=$(ray
          status --address ${RAY_ADDRESS} | grep -c node_)\nif [[ ! ${registered_node_count}
          -ge \"${PIPELINE_PARALLEL_SIZE}\" ]]; then\n  echo \"Unhealthy - Registered
          nodes count (${registered_node_count}) must not be less PIPELINE_PARALLEL_SIZE
          (${PIPELINE_PARALLEL_SIZE}).\"\n  exit 1\nfi     \n\n# Check model health\nhealth_check=$(curl
          -o /dev/null -s -w \"%{http_code}\\n\" http://localhost:8080/health)\nif
          [[ ${health_check} != 200 ]]; then\n  echo \"Unhealthy - vLLM Runtime Health
          Check failed.\" \n  exit 1\nfi    \n"
      failureThreshold: 2
      periodSeconds: 5
      successThreshold: 1
      timeoutSeconds: 15
    name: kserve-container
    ports:
    - containerPort: 8080
      name: http
      protocol: TCP
    readinessProbe:
      exec:
        command:
        - bash
        - -c
        - "# Check model health \nhealth_check=$(curl -o /dev/null -s -w \"%{http_code}\\n\"
          http://localhost:8080/health)\nif [[ ${health_check} != 200 ]]; then\n  echo
          \"Unhealthy - vLLM Runtime Health Check failed.\" \n  exit 1\nfi\n"
      failureThreshold: 2
      periodSeconds: 5
      successThreshold: 1
      timeoutSeconds: 15
    startupProbe:
      exec:
        command:
        - bash
        - -c
        - "# This need when head node have issues and restarted.\n# It will wait for
          new worker node.                     \nregistered_node_count=$(ray status
          --address ${RAY_ADDRESS} | grep -c node_)\nif [[ ! ${registered_node_count}
          -ge \"${PIPELINE_PARALLEL_SIZE}\" ]]; then\n  echo \"Unhealthy - Registered
          nodes count (${registered_node_count}) must not be less PIPELINE_PARALLEL_SIZE
          (${PIPELINE_PARALLEL_SIZE}).\"\n  exit 1\nfi\n\n# Double check to make sure
          Model is ready to serve.\nfor i in 1 2; do                    \n  # Check
          model health\n  health_check=$(curl -o /dev/null -s -w \"%{http_code}\\n\"
          http://localhost:8080/health)\n  if [[ ${health_check} != 200 ]]; then\n
          \   echo \"Unhealthy - vLLM Runtime Health Check failed.\" \n    exit 1\n
          \ fi\ndone\n"
      failureThreshold: 40
      initialDelaySeconds: 20
      periodSeconds: 30
      successThreshold: 1
      timeoutSeconds: 30
    volumeMounts:
    - mountPath: /dev/shm
      name: shm
    - mountPath: /etc/ray/tls
      name: ray-tls
  multiModel: false
  supportedModelFormats:
  - autoSelect: true
    name: vLLM
    priority: 2
  volumes:
  - emptyDir:
      medium: Memory
      sizeLimit: 12Gi
    name: shm
  - emptyDir: {}
    name: ray-tls
  - name: ray-tls-secret
    secret:
      secretName: ray-tls
  workerSpec:
    containers:
    - args:
      - "SECONDS=0\n\nwhile true; do              \n  if (( SECONDS <= 240 )); then\n
        \   if ray health-check --address \"${HEAD_SVC}.${POD_NAMESPACE}.svc.cluster.local:6379\"
        > /dev/null 2>&1; then\n      echo \"Global Control Service(GCS) is ready.\"\n
        \     break\n    fi\n    echo \"$SECONDS seconds elapsed: Waiting for Global
        Control Service(GCS) to be ready.\"\n  else\n    if ray health-check --address
        \"${HEAD_SVC}.${POD_NAMESPACE}.svc.cluster.local:6379\"; then\n      echo
        \"Global Control Service(GCS) is ready. Any error messages above can be safely
        ignored.\"\n      break\n    fi\n    echo \"$SECONDS seconds elapsed: Still
        waiting for Global Control Service(GCS) to be ready.\"\n    echo \"For troubleshooting,
        refer to the FAQ at https://docs.ray.io/en/master/cluster/kubernetes/troubleshooting/troubleshooting.html#kuberay-troubleshootin-guides\"\n
        \ fi\n\n  sleep 5\ndone\n\nexport RAY_HEAD_ADDRESS=\"${HEAD_SVC}.${POD_NAMESPACE}.svc.cluster.local:6379\"\necho
        \"Attempting to connect to Ray cluster at $RAY_HEAD_ADDRESS ...\"\nray start
        --address=\"${RAY_HEAD_ADDRESS}\" --block\n"
      command:
      - bash
      - -c
      env:
      - name: RAY_USE_TLS
        value: "1"
      - name: RAY_TLS_SERVER_CERT
        value: /etc/ray/tls/tls.pem
      - name: RAY_TLS_SERVER_KEY
        value: /etc/ray/tls/tls.pem
      - name: RAY_TLS_CA_CERT
        value: /etc/ray/tls/ca.crt
      - name: POD_NAME
        valueFrom:
          fieldRef:
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            fieldPath: metadata.namespace
      - name: POD_IP
        valueFrom:
          fieldRef:
            fieldPath: status.podIP
      image: quay.io/modh/vllm@sha256:79e1f24bba1d3e694f47f66ba9f8184e70310a10b77bf11c0febd0c926234950
      livenessProbe:
        exec:
          command:
          - bash
          - -c
          - |
            # Check if the registered nodes count matches PIPELINE_PARALLEL_SIZE
            registered_node_count=$(ray status --address ${HEAD_SVC}.${POD_NAMESPACE}.svc.cluster.local:6379 | grep -c node_)
            if [[ ! ${registered_node_count} -ge "${PIPELINE_PARALLEL_SIZE}" ]]; then
              echo "Unhealthy - Registered nodes count (${registered_node_count}) must not be less PIPELINE_PARALLEL_SIZE (${PIPELINE_PARALLEL_SIZE})."
              exit 1
            fi
        failureThreshold: 2
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 15
      name: worker-container
      resources:
        limits:
          cpu: "16"
          memory: 48Gi
        requests:
          cpu: "8"
          memory: 24Gi
      startupProbe:
        exec:
          command:
          - /bin/sh
          - -c
          - "registered_node_count=$(ray status --address ${HEAD_SVC}.${POD_NAMESPACE}.svc.cluster.local:6379
            | grep -c node_)\nif [[ ! ${registered_node_count} -ge \"${PIPELINE_PARALLEL_SIZE}\"
            ]]; then\n  echo \"Unhealthy - Registered nodes count (${registered_node_count})
            must not be less PIPELINE_PARALLEL_SIZE (${PIPELINE_PARALLEL_SIZE}).\"\n
            \ exit 1\nfi  \n\n# Double check to make sure Model is ready to serve.\nfor
            i in 1 2; do\n  # Check model health\n  model_health_check=$(curl -s ${HEAD_SVC}.${POD_NAMESPACE}.svc.cluster.local:8080/v1/models|grep
            -o ${ISVC_NAME})\n  if [[ ${model_health_check} != \"${ISVC_NAME}\" ]];
            then\n    echo \"Unhealthy - vLLM Runtime Health Check failed.\"\n    exit
            1\n  fi                     \n  sleep 10\ndone\n"
        failureThreshold: 40
        initialDelaySeconds: 20
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 30
      volumeMounts:
      - mountPath: /dev/shm
        name: shm
      - mountPath: /etc/ray/tls
        name: ray-tls
    pipelineParallelSize: 2
    tensorParallelSize: 2
    volumes:
    - emptyDir:
        medium: Memory
        sizeLimit: 12Gi
      name: shm
    - emptyDir: {}
      name: ray-tls
    - name: ray-tls-secret
      secret:
        secretName: ray-tls
