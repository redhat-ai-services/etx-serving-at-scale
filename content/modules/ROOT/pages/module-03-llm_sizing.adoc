= LLM GPU Requirements

When selecting hardware to run an LLM, several factors need to be considered including the model size, the KV Cache requirements for the use case, the use case concurrency requirements, the format of the model's data types, and any performance requirements such as Time to First Token (TTFT) or throughput.

== Estimating Model Size

One of the first calculations needed to help understand if a model will run on a particular GPU is creating an estimate of the model size.

The size of the model loaded into vRAM can be estimated using the following formula:

image::03-model-sizing-formula.png[Model Sizing Formula]


[cols="1,1"]
|===
| Symbol | Description
| M
| GPU memory

| P
| The number of parameters in the model


| 4b
| 4 bytes, expressing the bytes used for each parameter

| 32
| There are 32 bits in 4 bytes

| Q
| The amount of bits that should be used for loading the model. - 16 bits, 8 bits or 4 bits.

| 1.2
| Represents a 20% overhead of loading additional things in GPU memory.
|=== 

For example, for https://huggingface.co/ibm-granite/granite-3.3-8b-instruct/tree/main[Granite 3.3 8B Instruct] is 8 billion parameters and is an fp16 model.  

(((8*10^9*4)/(32/16))*1.2)/1024^3 = 17.9 Gb

For some additional details on this calculation, you can read more about it https://training.continuumlabs.ai/infrastructure/data-and-memory/calculating-gpu-memory-for-serving-llms[here].

== Estimating KV Cache

Beyond the model itself, vLLM also requires vRAM for the KV Cache.  The KV Cache requires a specific amount of vRAM per token that can be different from model to model.  An estimate for the vRAM per token can be calculated based on the architecture of the model with details from the config.json file on HuggingFace.

A great resource for better understanding how to calculate the vRAM per token, you can read more https://medium.com/@plienhar/llm-inference-series-4-kv-caching-a-deeper-look-4ba9a77746c8[here].

In addition to the vRAM per token, you will need to understand the context length requirements for the use case.  By default vLLM will use the max context length for the model, but if you are running the model on a smaller GPU, you may need to limit the KV Cache to a smaller context length.

Granite 3.3 8B Instruct requires 0.15625 Mb per token and the models max context length is 131,072, giving us a requirement of 20 Gb of vRAM for the KV Cache.

Along side our model requirements of about 17.9 Gb, that means we need about 38 Gb to run the model and support the max context length.

If we were trying to target a deployment on an L4 with 24 Gb of vRAM we could limit the max model length to around 35000 tokens which requires about 5 Gb.
