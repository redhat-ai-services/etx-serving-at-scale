= Distributed Serving

== Distributed Inference with vLLM

Serving large models often leads to memory bottlenecks, such as the dreaded CUDA out of memory error. To tackle this, there are two main solutions:

* *Reduce precision*: Utilizing FP8 and lower-bit quantization methods can reduce memory usage. However, this approach may impact accuracy and scalability, and is not sufficient by itself as models grow beyond hundreds of billions of parameters.

* *Distributed inference*: Spreading model computations across multiple GPUs or nodes enables scalability and efficiency. This is where distributed architectures like tensor parallelism and pipeline parallelism come into play.

=== vLLM Architecture and Large Language Model Inference Challenges

LLM inference poses unique challenges compared to training:

* Unlike training, which focuses purely on throughput with known static shapes, inference requires low latency and dynamic workload handling.
* Inference workloads must efficiently manage KV caches, speculative decoding, and prefill-to-decode transitions.
* Large models often exceed single-GPU capacity, requiring advanced parallelization strategies.

To address these issues, vLLM provides:

* *Tensor parallelism* to shard each model layer across multiple GPUs within a node.
* *Pipeline parallelism* to distribute contiguous sections of model layers across multiple nodes.
* *Data parallelism* to distribute data across multiple GPUs, with each GPU holding a copy of the model and processing different data portions concurrently.
* *Expert parallelism* to assign specific experts to dedicated GPUs, ensuring efficient utilization and reducing redundancy while distributing batched sequences between GPUs for the attention layers, avoiding KV cache duplication to improve memory efficiency.



In the next following sections, we will explore how to implement these parallelism techniques in vLLM for efficient distributed inference.

== Useful links

. Distributed Inference with vLLM - GPU Parallelism Techniques +
https://developers.redhat.com/articles/2025/02/06/distributed-inference-with-vllm#gpu_parallelism_techniques_in_vllm[^]

. How We Optimized vLLM DeepSeek R1 - Open Infra Week Contributions +
https://developers.redhat.com/articles/2025/03/19/how-we-optimized-vllm-deepseek-r1#open_infra_week_contributions[^]

== Existing lab resources

. GPU Partitioning Guide +
https://github.com/rh-aiservices-bu/gpu-partitioning-guide[^]

. Roadshow with GPUaaS +
https://github.com/odh-labs/rhoai-roadshow-v2[^]
